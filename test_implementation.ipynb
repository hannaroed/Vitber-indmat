{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for Ã¥ sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "<string>:3: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (Array(float64, 2, 'A', False, aligned=True), Array(float64, 2, 'C', False, aligned=True))\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 1\n",
    "n_max = 1\n",
    "m = 2\n",
    "n = 1\n",
    "\n",
    "d = 2\n",
    "k = 2\n",
    "p = 2\n",
    "\n",
    "r = 3\n",
    "L_1 = 1\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "self_attention = SelfAttention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m, False)\n",
    "softmax = Softmax()\n",
    "transformerblock = TransformerBlock(d,k,p)\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)  # (b, m, n)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = self_attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 1)\n",
      "(1, 2, 1)\n",
      "[[[0.62194674]\n",
      "  [0.37805325]]]\n",
      "(1, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hanna/Library/CloudStorage/OneDrive-NTNU/TMA4320 - Vitber/Vitber-indmat-5/layers.py:449: RuntimeWarning: Mean of empty slice.\n",
      "  per_sequence_loss = per_token_loss.mean(axis=1)\n",
      "/Users/hanna/micromamba/envs/3.12/lib/python3.12/site-packages/numpy/core/_methods.py:121: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1m\u001b[1m\u001b[1m- Resolution failure for literal arguments:\n\u001b[1mmissing a required argument: 'grad'\u001b[0m\n\u001b[0m\u001b[1m- Resolution failure for non-literal arguments:\n\u001b[1mNone\u001b[0m\n\u001b[0m\u001b[0m\n\u001b[0m\u001b[1mDuring: resolving callee type: BoundFunction((<class 'numba.core.types.misc.ClassInstanceType'>, 'backward') for instance.jitclass.NeuralNetwork#117c04980<embedding:instance.jitclass.EmbedPosition#118d0f5c0<embed:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,w:array(float64, 2d, A),params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>>,transformer_blocks:list(instance.jitclass.TransformerBlock#118d0f050<self_attention:instance.jitclass.SelfAttention#118cd3110<W_q:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_k:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_v:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_o:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,prev_A:OptionalType(array(float64, 3d, A)),softmax:instance.jitclass.Softmax#118cd3080<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,attention:instance.jitclass.Attention#118cd35c0<softmax:instance.jitclass.Softmax#118cd3080<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,matmul1:instance.jitclass.Matmul#118cd3140<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>,matmul2:instance.jitclass.Matmul#118cd3140<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>>>,feed_forward:instance.jitclass.FeedForward#118d0f650<l1:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,activation:instance.jitclass.Relu#118d0cb60<x:array(float64, 3d, A)>,l2:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,x:OptionalType(array(float64, 3d, A))>>)<iv=None>,lm_head:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,out_softmax:instance.jitclass.Softmax#118cd3080<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>>)\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n\u001b[0m\n\u001b[1m\nFile \"<string>\", line 3:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m grad_Z \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#and perform a backward pass\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m#and and do a gradient descent step\u001b[39;00m\n\u001b[1;32m     37\u001b[0m _ \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mstep_gd(\u001b[38;5;241m0.01\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/experimental/jitclass/boxing.py:61\u001b[0m, in \u001b[0;36m_generate_method.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/core/dispatcher.py:468\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    464\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    466\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 468\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/core/dispatcher.py:409\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1m\u001b[1m\u001b[1m- Resolution failure for literal arguments:\n\u001b[1mmissing a required argument: 'grad'\u001b[0m\n\u001b[0m\u001b[1m- Resolution failure for non-literal arguments:\n\u001b[1mNone\u001b[0m\n\u001b[0m\u001b[0m\n\u001b[0m\u001b[1mDuring: resolving callee type: BoundFunction((<class 'numba.core.types.misc.ClassInstanceType'>, 'backward') for instance.jitclass.NeuralNetwork#117c04980<embedding:instance.jitclass.EmbedPosition#118d0f5c0<embed:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,w:array(float64, 2d, A),params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>>,transformer_blocks:list(instance.jitclass.TransformerBlock#118d0f050<self_attention:instance.jitclass.SelfAttention#118cd3110<W_q:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_k:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_v:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_o:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,prev_A:OptionalType(array(float64, 3d, A)),softmax:instance.jitclass.Softmax#118cd3080<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,attention:instance.jitclass.Attention#118cd35c0<softmax:instance.jitclass.Softmax#118cd3080<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,matmul1:instance.jitclass.Matmul#118cd3140<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>,matmul2:instance.jitclass.Matmul#118cd3140<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>>>,feed_forward:instance.jitclass.FeedForward#118d0f650<l1:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,activation:instance.jitclass.Relu#118d0cb60<x:array(float64, 3d, A)>,l2:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,x:OptionalType(array(float64, 3d, A))>>)<iv=None>,lm_head:instance.jitclass.LinearLayer#118b5cbc0<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,out_softmax:instance.jitclass.Softmax#118cd3080<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>>)\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n\u001b[0m\n\u001b[1m\nFile \"<string>\", line 3:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "  # y_pred: (batch, m, n)\n",
    "        # y_true: (batch, n)\n",
    "        # m = number of classes\n",
    "\n",
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b,n_max))\n",
    "X = onehot(x, m)\n",
    "print(X.shape)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b,n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork(r, d, m, L_1, p, k)\n",
    "\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X)\n",
    "print(Z.shape)\n",
    "\n",
    "#compute the loss\n",
    "print(Z)\n",
    "print(y.shape)\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "\n",
    "#and perform a backward pass\n",
    "_ = network.backward()\n",
    "\n",
    "#and and do a gradient descent step\n",
    "_ = network.step_gd(0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "L=[nan], expected L>=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#check if loss is non-negative\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m L \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected L>=0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m grad_Z\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m Z\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_Z.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_Z\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mZ\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#check if onehot(y) gives zero loss\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: L=[nan], expected L>=0"
     ]
    }
   ],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f5dfc9faa95b001f0a483cdde4e36b57b705826fc04f26cec8b0bea8b825bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
