{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing av koden\n",
    "\n",
    "For å se om alt fungerer som det skal, har vi implementert tester av forskjellige lag og funksjoner.\n",
    "Vi bruker blandt annet den innebgydde funksjonen assert som sjekker om påstanden til venstre er korrekt, dersom den ikke er det returnerer den AssertionError med kommentaren til høyre for komma. \n",
    "\n",
    "For eksempel har vi:\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "Dersom X har dimensjon (b,m,n) vil koden kjøre normalt, dersom den ikke har det vil den returnere AssertionError og \"X.shape = {X.shape}, expected {(b,m,n)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import _jit_onehot, onehot\n",
    "import numpy as np\n",
    "from optimizer import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "<string>:3: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (Array(float64, 2, 'A', False, aligned=True), Array(float64, 2, 'C', False, aligned=True))\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z0 dim: (6, 10, 5)\n",
      "z1 dim: (6, 10, 5)\n",
      "z2 dim: (6, 10, 5)\n",
      "z3 dim: (6, 8, 5)\n",
      "Z dim: (6, 8, 5)\n"
     ]
    }
   ],
   "source": [
    "# We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#r = 3\n",
    "#L_1 = 2\n",
    "\n",
    "# Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "# initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "self_attention = SelfAttention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m, True)\n",
    "softmax = Softmax()\n",
    "transformerblock = TransformerBlock(d,k,p)\n",
    "\n",
    "\n",
    "# a manual forward pass\n",
    "X = _jit_onehot(x, m)  # (b, m, n)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = self_attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "print(f'z0 dim: {z0.shape}')\n",
    "print(f'z1 dim: {z1.shape}')\n",
    "print(f'z2 dim: {z2.shape}')\n",
    "print(f'z3 dim: {z3.shape}')\n",
    "print(f'Z dim: {Z.shape}')\n",
    "\n",
    "\n",
    "# Check the shapes\n",
    "assert X.shape == (b,m,n), f'X.shape={X.shape}, expected {(b,m,n)}'\n",
    "assert z0.shape == (b,d,n), f'z0.shape={z0.shape}, expected {(b,d,n)}'\n",
    "assert z1.shape == (b,d,n), f'z1.shape={z1.shape}, expected {(b,d,n)}'\n",
    "assert z2.shape == (b,d,n), f'z2.shape={z2.shape}, expected {(b,d,n)}'\n",
    "assert z3.shape == (b,m,n), f'z3.shape={z3.shape}, expected {(b,m,n)}'\n",
    "assert Z.shape == (b,m,n), f'Z.shape={Z.shape}, expected {(b,m,n)}'\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f'X.sum()={X.sum()}, expected {b*n}'\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f'Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}'\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f'Z.sum()={Z.sum()}, expected {b*n}'\n",
    "assert np.all(Z>=0), f'Z={Z}, expected all entries to be non-negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x (6, 5)\n",
      "The dimension of X is: (6, 8, 5)\n",
      "The dimension of y is: (6, 6)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m CrossEntropy()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# do a forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dimension of Z is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mZ\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# compute the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/experimental/jitclass/boxing.py:61\u001b[0m, in \u001b[0;36m_generate_method.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/TMA4320 - Vitber/Vitber-indmat-11/utils.py:144\u001b[0m, in \u001b[0;36mbatched_mm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m ao, ai \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    143\u001b[0m ab, bi, bo \u001b[38;5;241m=\u001b[39m B\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ai \u001b[38;5;241m==\u001b[39m bi\n\u001b[1;32m    145\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((ab, ao, bo))\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nb\u001b[38;5;241m.\u001b[39mprange(ab):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# y_pred: (batch, m, n)\n",
    "# y_true: (batch, m, n)\n",
    "# m = number of classes\n",
    "\n",
    "# test the forward pass\n",
    "\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "print(f'shape of x {x.shape}')\n",
    "X = jit_onehot(x, m) # dim (b,m,n)\n",
    "print(f'The dimension of X is: {X.shape}')\n",
    "\n",
    "# we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b, n_y))\n",
    "print(f'The dimension of y is: {y.shape}')\n",
    "\n",
    "\n",
    "# initialize a neural network based on the layers above\n",
    "network = NeuralNetwork(r=2, d=10, m=2, L=2, p=20, k=5)\n",
    "\n",
    "# and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "# do a forward pass\n",
    "Z = network.forward(X)\n",
    "print(f'The dimension of Z is: {Z.shape}')\n",
    "\n",
    "# compute the loss\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "# get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "\n",
    "# Does not work on our network because we did not save our gradient in a array and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "\n",
    "_ = network.step_gd(Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypingError",
     "evalue": "Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1m('wrong tuple length for $6load_attr.1: ', 'expected 2, got 3')\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of exhaust iter at /Users/hanna/Library/CloudStorage/OneDrive-NTNU/TMA4320 - Vitber/Vitber-indmat-11/utils.py (71)\u001b[0m\n\u001b[1m\nFile \"utils.py\", line 71:\u001b[0m\n\u001b[1mdef _jit_onehot(x, m):\n    <source elided>\n    '''\n\u001b[1m    b, n = x.shape\n\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypingError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 135\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    134\u001b[0m    test_forward_shape()\n\u001b[0;32m--> 135\u001b[0m    \u001b[43mtest_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m    test_adam()\n\u001b[1;32m    137\u001b[0m    \u001b[38;5;66;03m#test_backward_correct()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m, in \u001b[0;36mtest_backward\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m y_true \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x)\n\u001b[1;32m     44\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mCrossEntropy()\n\u001b[0;32m---> 45\u001b[0m Y_true \u001b[38;5;241m=\u001b[39m \u001b[43mjit_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m Y_true_pad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(batch_x, ((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (Y_pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m     48\u001b[0m loss_function\u001b[38;5;241m.\u001b[39mforward(Y_pred, Y_true_pad)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/TMA4320 - Vitber/Vitber-indmat-11/utils.py:80\u001b[0m, in \u001b[0;36mjit_onehot\u001b[0;34m(x, m)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjit_onehot\u001b[39m(x, m):\n\u001b[1;32m     79\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_onehot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/core/dispatcher.py:468\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    464\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThis error may have been caused \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby the following argument(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00margs_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    466\u001b[0m         e\u001b[38;5;241m.\u001b[39mpatch_message(msg)\n\u001b[0;32m--> 468\u001b[0m     \u001b[43merror_rewrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtyping\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mUnsupportedError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# Something unsupported is present in the user code, add help info\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     error_rewrite(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsupported_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/core/dispatcher.py:409\u001b[0m, in \u001b[0;36m_DispatcherBase._compile_for_args.<locals>.error_rewrite\u001b[0;34m(e, issue_type)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypingError\u001b[0m: Failed in nopython mode pipeline (step: nopython frontend)\n\u001b[1m\u001b[1m('wrong tuple length for $6load_attr.1: ', 'expected 2, got 3')\u001b[0m\n\u001b[0m\u001b[1mDuring: typing of exhaust iter at /Users/hanna/Library/CloudStorage/OneDrive-NTNU/TMA4320 - Vitber/Vitber-indmat-11/utils.py (71)\u001b[0m\n\u001b[1m\nFile \"utils.py\", line 71:\u001b[0m\n\u001b[1mdef _jit_onehot(x, m):\n    <source elided>\n    '''\n\u001b[1m    b, n = x.shape\n\u001b[0m    \u001b[1m^\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from typing import Any\n",
    "import layers as l\n",
    "from optimizer import Adam\n",
    "from utils import *\n",
    "from training import make_model, training_sorting, training_addition\n",
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "   data_set = get_train_test_sorting(length=5, num_ints=2, samples_per_batch=250, n_batches_train=10, n_batches_test=2)\n",
    "   train_set = list(zip(data_set['x_train'], data_set['y_train']))\n",
    "   return train_set\n",
    "\n",
    "\n",
    "def test_forward_shape():\n",
    "   model = make_model()\n",
    "\n",
    "   train_set = get_test_data()\n",
    "\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   out = model.forward(batch_x)\n",
    "   assert out.shape == (250, 2, 9)\n",
    "\n",
    "\n",
    "def test_backward():\n",
    "   model = make_model()\n",
    "   grad_loss = np.random.randn(250)\n",
    "   train_set = get_test_data()\n",
    "\n",
    "   batch_x = jit_onehot(train_set[0][0], 2)\n",
    "   Y_pred = model.forward(batch_x)\n",
    "   loss_function = l.CrossEntropy()\n",
    "   y_true = train_set[1][0]\n",
    "   Y_true = jit_onehot(y_true, 2)\n",
    "   Y_true_pad = np.pad(batch_x, ((0, 0), (0, 0), (Y_pred.shape[2] - batch_x.shape[2], 0)))\n",
    "\n",
    "   loss_function.forward(Y_pred, Y_true_pad)\n",
    "   grad_loss = loss_function.backward()\n",
    "   model.backward(grad_loss)\n",
    "  \n",
    "\n",
    "def test_adam():\n",
    "   np.seterr(all='raise')\n",
    "\n",
    "   # Initialize model and optimizer\n",
    "   model = make_model()\n",
    "   optimizer = Adam()\n",
    "   # Overfit on a single example\n",
    "\n",
    "   # Get all training data\n",
    "   train_set = get_test_data()\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "\n",
    "   m = 2\n",
    "\n",
    "   # First input value from training set\n",
    "   input = train_set[0][0]\n",
    "   output = train_set[0][1]\n",
    "   batch_x = jit_onehot(input, m)\n",
    "\n",
    "   for _ in range(1000):\n",
    "      y_hat = model.forward(batch_x)\n",
    "      # y_hat: (b, m, n)\n",
    "      \n",
    "      y_hat_indices = np.argmax(y_hat, axis=1)\n",
    "      Y_pred\n",
    "\n",
    "      Y_pred_slice = y_hat[:,:,-Y_true.shape[2]:]\n",
    "      correct = y_hat_indices == output\n",
    "      accuracy = np.mean(correct)\n",
    "\n",
    "      # y_true is not one-hot encoded, but instead class indices\n",
    "      loss_value = loss_function.forward(y_hat, y_true=train_set[1][0]).mean()\n",
    "\n",
    "\n",
    "      # dLdY: (b, m, n)\n",
    "      grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "      model.backward(grad_loss)\n",
    "\n",
    "\n",
    "      model.step_gd(optimizer)\n",
    "\n",
    "\n",
    "      print(f'{accuracy=:.5f}, {loss_value=:.5f}')\n",
    "\n",
    "\n",
    "def module_backward_works(input, out_shape: tuple, module):\n",
    "   # Not done, work in progress\n",
    "\n",
    "\n",
    "   # Want dY/dX of this value\n",
    "   grad_output = np.ones(out_shape)\n",
    "\n",
    "\n",
    "   # Compute the forward pass\n",
    "   forward_result = module.forward(input)\n",
    "\n",
    "\n",
    "   # Now do backward with this in mind\n",
    "   dL_dx = module.backward(grad_output)\n",
    "\n",
    "\n",
    "   perturbed = input + delta_input\n",
    "   forward_perturbed = module.forward(perturbed)\n",
    "   # print(forward_perturbed)\n",
    "   print(((forward_perturbed - forward_result).sum() / delta))\n",
    "   # print(grad_output)\n",
    "   # assert np.allclose((forward_perturbed - forward_result) / delta, grad_output, atol=1e-6)\n",
    "\n",
    "def test_backward_correct():\n",
    "   batch_size = 10\n",
    "   in_dims = 2\n",
    "   out_dims = 3\n",
    "   seq_len = 5\n",
    "   module = l.LinearLayer(in_dims, out_dims, has_bias=False)\n",
    "   input = np.random.randn(batch_size, in_dims, seq_len)\n",
    "   module_backward_works(input, (batch_size, out_dims, seq_len), module)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   test_forward_shape()\n",
    "   test_backward()\n",
    "   test_adam()\n",
    "   #test_backward_correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f5dfc9faa95b001f0a483cdde4e36b57b705826fc04f26cec8b0bea8b825bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
