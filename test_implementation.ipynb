{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing av koden\n",
    "\n",
    "For å se om alt fungerer som det skal, har vi implementert tester av forskjellige lag og funksjoner.\n",
    "Vi bruker blandt annet den innebgydde funksjonen assert som sjekker om påstanden til venstre er korrekt, dersom den ikke er det returnerer den AssertionError med kommentaren til høyre for komma. \n",
    "\n",
    "For eksempel har vi:\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "Dersom X har dimensjon (b,m,n) vil koden kjøre normalt, dersom den ikke har det vil den returnere AssertionError og \"X.shape = {X.shape}, expected {(b,m,n)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import _jit_onehot, onehot\n",
    "import numpy as np\n",
    "from optimizer import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z0 dim: (6, 10, 5)\n",
      "z1 dim: (6, 10, 5)\n",
      "z2 dim: (6, 10, 5)\n",
      "z3 dim: (6, 8, 5)\n",
      "Z dim: (6, 8, 5)\n"
     ]
    }
   ],
   "source": [
    "# We choose some arbitrary values for the dimensions\n",
    "b = 6\n",
    "n_max = 7\n",
    "m = 8\n",
    "n = 5\n",
    "\n",
    "d = 10\n",
    "k = 5\n",
    "p = 20\n",
    "\n",
    "#r = 3\n",
    "#L_1 = 2\n",
    "\n",
    "# Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "# initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "self_attention = SelfAttention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m, True)\n",
    "softmax = Softmax()\n",
    "transformerblock = TransformerBlock(d,k,p)\n",
    "\n",
    "\n",
    "# a manual forward pass\n",
    "X = _jit_onehot(x, m)  # (b, m, n)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = self_attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "print(f'z0 dim: {z0.shape}')\n",
    "print(f'z1 dim: {z1.shape}')\n",
    "print(f'z2 dim: {z2.shape}')\n",
    "print(f'z3 dim: {z3.shape}')\n",
    "print(f'Z dim: {Z.shape}')\n",
    "\n",
    "\n",
    "\n",
    "# Check the shapes\n",
    "assert X.shape == (b,m,n), f'X.shape={X.shape}, expected {(b,m,n)}'\n",
    "assert z0.shape == (b,d,n), f'z0.shape={z0.shape}, expected {(b,d,n)}'\n",
    "assert z1.shape == (b,d,n), f'z1.shape={z1.shape}, expected {(b,d,n)}'\n",
    "assert z2.shape == (b,d,n), f'z2.shape={z2.shape}, expected {(b,d,n)}'\n",
    "assert z3.shape == (b,m,n), f'z3.shape={z3.shape}, expected {(b,m,n)}'\n",
    "assert Z.shape == (b,m,n), f'Z.shape={Z.shape}, expected {(b,m,n)}'\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f'X.sum()={X.sum()}, expected {b*n}'\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f'Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}'\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f'Z.sum()={Z.sum()}, expected {b*n}'\n",
    "assert np.all(Z>=0), f'Z={Z}, expected all entries to be non-negative'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x (6, 5)\n",
      "The dimension of X is: (6, 8, 5)\n",
      "The dimension of y is: (6, 8, 5)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m CrossEntropy()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# do a forward pass\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m Z \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mforward(X)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe dimension of Z is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mZ\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/3.12/lib/python3.12/site-packages/numba/experimental/jitclass/boxing.py:61\u001b[0m, in \u001b[0;36m_generate_method.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/4Semester/Vitenskapeligeberegninger/Vitber-indmat/utils.py:144\u001b[0m, in \u001b[0;36mbatched_mm\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m ao, ai \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mshape\n\u001b[1;32m    143\u001b[0m ab, bi, bo \u001b[39m=\u001b[39m B\u001b[39m.\u001b[39mshape\n\u001b[0;32m--> 144\u001b[0m \u001b[39massert\u001b[39;00m ai \u001b[39m==\u001b[39m bi\n\u001b[1;32m    145\u001b[0m out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((ab, ao, bo))\n\u001b[1;32m    146\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m nb\u001b[39m.\u001b[39mprange(ab):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# y_pred: (batch, m, n)\n",
    "# y_true: (batch, m, n)\n",
    "# m = number of classes\n",
    "\n",
    "# test the forward pass\n",
    "\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "print(f'shape of x {x.shape}')\n",
    "X = onehot(x, m) # dim (b,m,n)\n",
    "print(f'The dimension of X is: {X.shape}')\n",
    "\n",
    "# we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0,m,(b, m, n))\n",
    "print(f'The dimension of y is: {y.shape}')\n",
    "\n",
    "\n",
    "# initialize a neural network based on the layers above\n",
    "network = NeuralNetwork(r=2, d=10, m=2, L=2, p=20, k=5)\n",
    "\n",
    "# and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "# do a forward pass\n",
    "Z = network.forward(X)\n",
    "print(f'The dimension of Z is: {Z.shape}')\n",
    "\n",
    "# compute the loss\n",
    "#L = loss.forward(Z, y)\n",
    "\n",
    "# get the derivative of the loss wrt Z\n",
    "#grad_Z = loss.backward()\n",
    "\n",
    "# Does not work on our network because we did not save our gradient in a array\n",
    "#and perform a backward pass\n",
    "# _ = network.backward(grad_Z)\n",
    "\n",
    "#and and do a gradient descent step\n",
    "\n",
    "#_ = network.step_gd(Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'training' from 'training' (/Users/karineagrande/Documents/4Semester/Vitenskapeligeberegninger/Vitber-indmat/training.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01ml\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onehot\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_model, training\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_generators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_train_test_sorting\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_test_data\u001b[39m():\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'training' from 'training' (/Users/karineagrande/Documents/4Semester/Vitenskapeligeberegninger/Vitber-indmat/training.py)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from typing import Any\n",
    "import layers as l\n",
    "from utils import onehot\n",
    "from training import make_model, training\n",
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "   data_set = get_train_test_sorting(length=5, num_ints=2, samples_per_batch=250, n_batches_train=10, n_batches_test=2)\n",
    "   train_set = list(zip(data_set['x_train'], data_set['y_train']))\n",
    "   return train_set\n",
    "\n",
    "\n",
    "def test_forward_shape():\n",
    "   model = make_model()\n",
    "\n",
    "   train_set = get_test_data()\n",
    "\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   out = model.forward(batch_x)\n",
    "   assert out.shape == (250, 2, 9)\n",
    "\n",
    "\n",
    "def test_backward():\n",
    "   model = make_model()\n",
    "   grad_loss = np.random.randn(250)\n",
    "\n",
    "\n",
    "   train_set = get_test_data()\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   y_hat = model.forward(batch_x)\n",
    "\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "   loss_function.forward(y_hat, y_true=train_set[1][0])\n",
    "   grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "   model.backward(grad_loss)\n",
    "  \n",
    "\n",
    "def test_adam():\n",
    "   np.seterr(all='raise')\n",
    "\n",
    "\n",
    "   # Initialize model and optimizer\n",
    "   model = make_model()\n",
    "   optimizer = l.Adam()\n",
    "   # Overfit on a single example\n",
    "\n",
    "\n",
    "   # Get all training data\n",
    "   train_set = get_test_data()\n",
    "\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "\n",
    "\n",
    "   m = 2\n",
    "\n",
    "\n",
    "   # First input value from training set\n",
    "   input = train_set[0][0]\n",
    "   output = train_set[0][1]\n",
    "   batch_x = onehot(input, m)\n",
    "\n",
    "\n",
    "   for _ in range(1000):\n",
    "       y_hat = model.forward(batch_x)\n",
    "       # y_hat: (b, m, n)\n",
    "       y_hat_indices = np.argmax(y_hat, axis=1)\n",
    "\n",
    "\n",
    "       correct = y_hat_indices == output\n",
    "       accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "       # y_true is not one-hot encoded, but instead class indices\n",
    "       loss_value = loss_function.forward(y_hat, y_true=train_set[1][0]).mean()\n",
    "\n",
    "\n",
    "       # dLdY: (b, m, n)\n",
    "       grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       model.backward(grad_loss)\n",
    "\n",
    "\n",
    "       model.step_gd(optimizer)\n",
    "\n",
    "\n",
    "       print(f'{accuracy=:.5f}, {loss_value=:.5f}')\n",
    "\n",
    "\n",
    "def module_backward_works(input, out_shape: tuple, module):\n",
    "   # Not done, work in progress\n",
    "\n",
    "\n",
    "   # Want dY/dX of this value\n",
    "   grad_output = np.ones(out_shape)\n",
    "\n",
    "\n",
    "   # Compute the forward pass\n",
    "   forward_result = module.forward(input)\n",
    "\n",
    "\n",
    "   # Now do backward with this in mind\n",
    "   dL_dx = module.backward(grad_output)\n",
    "\n",
    "\n",
    "   perturbed = input + delta_input\n",
    "   forward_perturbed = module.forward(perturbed)\n",
    "   # print(forward_perturbed)\n",
    "   print(((forward_perturbed - forward_result).sum() / delta))\n",
    "   # print(grad_output)\n",
    "   # assert np.allclose((forward_perturbed - forward_result) / delta, grad_output, atol=1e-6)\n",
    "\n",
    "\n",
    "def test_backward_correct():\n",
    "   batch_size = 10\n",
    "   in_dims = 2\n",
    "   out_dims = 3\n",
    "   seq_len = 5\n",
    "   module = l.LinearLayer(in_dims, out_dims, has_bias=False)\n",
    "   input = np.random.randn(batch_size, in_dims, seq_len)\n",
    "   module_backward_works(input, (batch_size, out_dims, seq_len), module)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   # test_forward_shape()\n",
    "   # test_backward()\n",
    "   test_adam()\n",
    "   # test_backward_correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f5dfc9faa95b001f0a483cdde4e36b57b705826fc04f26cec8b0bea8b825bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
