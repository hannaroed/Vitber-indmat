{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test om koden er riktig implementert\n",
    "\n",
    "Her er et forslag til testfunksjoner for Ã¥ sjekke om koden er riktig implementert.\n",
    "```assert variabel``` vil gi en feilmelding med mindre variabelen ```variabel = True```. For eksempel vil ```assert a == b``` gi en feilmelding med mindre ```a``` og ```b``` er like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from neural_network import NeuralNetwork\n",
    "from utils import onehot\n",
    "import numpy as np\n",
    "from optimizer import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:3: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (Array(float64, 2, 'A', False, aligned=True), Array(float64, 2, 'C', False, aligned=True))\n"
     ]
    }
   ],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 2\n",
    "n_max = 5\n",
    "m = 2\n",
    "n = 1\n",
    "\n",
    "d = 7\n",
    "k = 3\n",
    "p = 10\n",
    "\n",
    "r = 3\n",
    "L_1 = 1\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "self_attention = SelfAttention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m, False)\n",
    "softmax = Softmax()\n",
    "transformerblock = TransformerBlock(d,k,p)\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)  # (b, m, n)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = self_attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 5)\n",
      "(2, 2, 5)\n",
      "[[[0.47041517 0.46532783 0.46937922 0.47122817 0.4826279 ]\n",
      "  [0.52958483 0.53467216 0.53062078 0.52877183 0.5173721 ]]\n",
      "\n",
      " [[0.45490703 0.46548663 0.485104   0.45564406 0.46686848]\n",
      "  [0.54509296 0.53451337 0.514896   0.54435594 0.53313152]]]\n",
      "This is y:\n",
      "(2, 4)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "  # y_pred: (batch, m, n)\n",
    "        # y_true: (batch, n)\n",
    "        # m = number of classes\n",
    "\n",
    "\n",
    "#test the forward pass\n",
    "x = np.random.randint(0, m, (b, n_max))\n",
    "X = onehot(x, m)\n",
    "print(X.shape)\n",
    "\n",
    "#we test with a y that is shorter than the maximum length\n",
    "n_y = n_max - 1\n",
    "y = np.random.randint(0, m, (b, n_y))\n",
    "\n",
    "#initialize a neural network based on the layers above\n",
    "network = NeuralNetwork(r, d, m, L_1, p, k)\n",
    "\n",
    "#and a loss function\n",
    "loss = CrossEntropy()\n",
    "\n",
    "#do a forward pass\n",
    "Z = network.forward(X)\n",
    "print(Z.shape)\n",
    "\n",
    "#compute the loss\n",
    "print(Z)\n",
    "print('This is y:')\n",
    "print(y.shape)\n",
    "L = loss.forward(Z, y)\n",
    "\n",
    "#get the derivative of the loss wrt Z\n",
    "grad_Z = loss.backward()\n",
    "\n",
    "'''\n",
    "# Does not work on our network because we did not save our gradient in a array\n",
    "#and perform a backward pass\n",
    "_ = network.backward(grad_Z)\n",
    "'''\n",
    "#and and do a gradient descent step\n",
    "\n",
    "#_ = network.step_gd(Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from typing import Any\n",
    "import layers as l\n",
    "from utils import onehot\n",
    "from training import make_model, training\n",
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "   data_set = get_train_test_sorting(length=5, num_ints=2, samples_per_batch=250, n_batches_train=10, n_batches_test=2)\n",
    "   train_set = list(zip(data_set['x_train'], data_set['y_train']))\n",
    "   return train_set\n",
    "\n",
    "\n",
    "def test_forward_shape():\n",
    "   model = make_model()\n",
    "\n",
    "   train_set = get_test_data()\n",
    "\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   out = model.forward(batch_x)\n",
    "   assert out.shape == (250, 2, 9)\n",
    "\n",
    "\n",
    "def test_backward():\n",
    "   model = make_model()\n",
    "   grad_loss = np.random.randn(250)\n",
    "\n",
    "\n",
    "   train_set = get_test_data()\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   y_hat = model.forward(batch_x)\n",
    "\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "   loss_function.forward(y_hat, y_true=train_set[1][0])\n",
    "   grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "   model.backward(grad_loss)\n",
    "  \n",
    "\n",
    "def test_adam():\n",
    "   np.seterr(all='raise')\n",
    "\n",
    "\n",
    "   # Initialize model and optimizer\n",
    "   model = make_model()\n",
    "   optimizer = l.Adam()\n",
    "   # Overfit on a single example\n",
    "\n",
    "\n",
    "   # Get all training data\n",
    "   train_set = get_test_data()\n",
    "\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "\n",
    "\n",
    "   m = 2\n",
    "\n",
    "\n",
    "   # First input value from training set\n",
    "   input = train_set[0][0]\n",
    "   output = train_set[0][1]\n",
    "   batch_x = onehot(input, m)\n",
    "\n",
    "\n",
    "   for _ in range(1000):\n",
    "       y_hat = model.forward(batch_x)\n",
    "       # y_hat: (b, m, n)\n",
    "       y_hat_indices = np.argmax(y_hat, axis=1)\n",
    "\n",
    "\n",
    "       correct = y_hat_indices == output\n",
    "       accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "       # y_true is not one-hot encoded, but instead class indices\n",
    "       loss_value = loss_function.forward(y_hat, y_true=train_set[1][0]).mean()\n",
    "\n",
    "\n",
    "       # dLdY: (b, m, n)\n",
    "       grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       model.backward(grad_loss)\n",
    "\n",
    "\n",
    "       model.step_gd(optimizer)\n",
    "\n",
    "\n",
    "       print(f'{accuracy=:.5f}, {loss_value=:.5f}')\n",
    "\n",
    "\n",
    "def module_backward_works(input, out_shape: tuple, module):\n",
    "   # Not done, work in progress\n",
    "\n",
    "\n",
    "   # Want dY/dX of this value\n",
    "   grad_output = np.ones(out_shape)\n",
    "\n",
    "\n",
    "   # Compute the forward pass\n",
    "   forward_result = module.forward(input)\n",
    "\n",
    "\n",
    "   # Now do backward with this in mind\n",
    "   dL_dx = module.backward(grad_output)\n",
    "\n",
    "\n",
    "   perturbed = input + delta_input\n",
    "   forward_perturbed = module.forward(perturbed)\n",
    "   # print(forward_perturbed)\n",
    "   print(((forward_perturbed - forward_result).sum() / delta))\n",
    "   # print(grad_output)\n",
    "   # assert np.allclose((forward_perturbed - forward_result) / delta, grad_output, atol=1e-6)\n",
    "\n",
    "\n",
    "def test_backward_correct():\n",
    "   batch_size = 10\n",
    "   in_dims = 2\n",
    "   out_dims = 3\n",
    "   seq_len = 5\n",
    "   module = l.LinearLayer(in_dims, out_dims, has_bias=False)\n",
    "   input = np.random.randn(batch_size, in_dims, seq_len)\n",
    "   module_backward_works(input, (batch_size, out_dims, seq_len), module)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   # test_forward_shape()\n",
    "   # test_backward()\n",
    "   test_adam()\n",
    "   # test_backward_correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if loss is non-negative\n",
    "assert L >= 0, f\"L={L}, expected L>=0\"\n",
    "assert grad_Z.shape == Z.shape, f\"grad_Z.shape={grad_Z.shape}, expected {Z.shape}\"\n",
    "\n",
    "#check if onehot(y) gives zero loss\n",
    "Y = onehot(y, m)\n",
    "L = loss.forward(Y, y)\n",
    "assert L < 1e-5, f\"L={L}, expected L<1e-5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For eksempel:\n",
    "variable = True\n",
    "assert variable, \"You need to change 'variable' to True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "<string>:3: NumbaPerformanceWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1m\u001b[1m'@' is faster on contiguous arrays, called on (Array(float64, 2, 'A', False, aligned=True), Array(float64, 2, 'C', False, aligned=True))\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#We choose some arbitrary values for the dimensions\n",
    "b = 2\n",
    "n_max = 5\n",
    "m = 2\n",
    "n = 1\n",
    "\n",
    "d = 7\n",
    "k = 3\n",
    "p = 10\n",
    "\n",
    "r = 3\n",
    "L_1 = 1\n",
    "\n",
    "#Create an arbitrary dataset\n",
    "x = np.random.randint(0, m, (b,n))\n",
    "y = np.random.randint(0, m, (b,n_max))\n",
    "\n",
    "#initialize the layers\n",
    "feed_forward = FeedForward(d,p)\n",
    "self_attention = SelfAttention(d,k)\n",
    "embed_pos = EmbedPosition(n_max,m,d)\n",
    "un_embed = LinearLayer(d,m, False)\n",
    "softmax = Softmax()\n",
    "transformerblock = TransformerBlock(d,k,p)\n",
    "\n",
    "\n",
    "#a manual forward pass\n",
    "X = onehot(x, m)  # (b, m, n)\n",
    "z0 = embed_pos.forward(X)\n",
    "z1 = feed_forward.forward(z0)\n",
    "z2 = self_attention.forward(z1)\n",
    "z3 = un_embed.forward(z2)\n",
    "Z = softmax.forward(z3) \n",
    "\n",
    "\n",
    "\n",
    "#check the shapes\n",
    "assert X.shape == (b,m,n), f\"X.shape={X.shape}, expected {(b,m,n)}\"\n",
    "assert z0.shape == (b,d,n), f\"z0.shape={z0.shape}, expected {(b,d,n)}\"\n",
    "assert z1.shape == (b,d,n), f\"z1.shape={z1.shape}, expected {(b,d,n)}\"\n",
    "assert z2.shape == (b,d,n), f\"z2.shape={z2.shape}, expected {(b,d,n)}\"\n",
    "assert z3.shape == (b,m,n), f\"z3.shape={z3.shape}, expected {(b,m,n)}\"\n",
    "assert Z.shape == (b,m,n), f\"Z.shape={Z.shape}, expected {(b,m,n)}\"\n",
    "\n",
    "#is X one-hot?\n",
    "assert X.sum() == b*n, f\"X.sum()={X.sum()}, expected {b*n}\"\n",
    "\n",
    "\n",
    "assert np.allclose(Z.sum(axis=1), 1), f\"Z.sum(axis=1)={Z.sum(axis=1)}, expected {np.ones(b)}\"\n",
    "assert np.abs(Z.sum() - b*n) < 1e-5, f\"Z.sum()={Z.sum()}, expected {b*n}\"\n",
    "assert np.all(Z>=0), f\"Z={Z}, expected all entries to be non-negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHere you may add additional tests to for example:\\n\\n- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\\n- Check if the parameters change when you perform a gradient descent step\\n- Check if the loss decreases when you perform a gradient descent step\\n\\nThis is voluntary, but could be useful.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here you may add additional tests to for example:\n",
    "\n",
    "- Check if the ['d'] keys in the parameter dictionaries are not None, or receive something when running backward pass\n",
    "- Check if the parameters change when you perform a gradient descent step\n",
    "- Check if the loss decreases when you perform a gradient descent step\n",
    "\n",
    "This is voluntary, but could be useful.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from typing import Any\n",
    "import layers as l\n",
    "from utils import onehot\n",
    "from training import make_model, training\n",
    "from data_generators import get_train_test_sorting\n",
    "\n",
    "\n",
    "def get_test_data():\n",
    "   data_set = get_train_test_sorting(length=5, num_ints=2, samples_per_batch=250, n_batches_train=10, n_batches_test=2)\n",
    "   train_set = list(zip(data_set['x_train'], data_set['y_train']))\n",
    "   return train_set\n",
    "\n",
    "\n",
    "def test_forward_shape():\n",
    "   model = make_model()\n",
    "\n",
    "   train_set = get_test_data()\n",
    "\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   out = model.forward(batch_x)\n",
    "   assert out.shape == (250, 2, 9)\n",
    "\n",
    "\n",
    "def test_backward():\n",
    "   model = make_model()\n",
    "   grad_loss = np.random.randn(250)\n",
    "\n",
    "\n",
    "   train_set = get_test_data()\n",
    "   batch_x = onehot(train_set[0][0], 2)\n",
    "   y_hat = model.forward(batch_x)\n",
    "\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "   loss_function.forward(y_hat, y_true=train_set[1][0])\n",
    "   grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "   model.backward(grad_loss)\n",
    "  \n",
    "\n",
    "def test_adam():\n",
    "   np.seterr(all='raise')\n",
    "\n",
    "\n",
    "   # Initialize model and optimizer\n",
    "   model = make_model()\n",
    "   optimizer = l.Adam()\n",
    "   # Overfit on a single example\n",
    "\n",
    "\n",
    "   # Get all training data\n",
    "   train_set = get_test_data()\n",
    "\n",
    "\n",
    "   loss_function = l.CrossEntropy()\n",
    "\n",
    "\n",
    "   m = 2\n",
    "\n",
    "\n",
    "   # First input value from training set\n",
    "   input = train_set[0][0]\n",
    "   output = train_set[0][1]\n",
    "   batch_x = onehot(input, m)\n",
    "\n",
    "\n",
    "   for _ in range(1000):\n",
    "       y_hat = model.forward(batch_x)\n",
    "       # y_hat: (b, m, n)\n",
    "       y_hat_indices = np.argmax(y_hat, axis=1)\n",
    "\n",
    "\n",
    "       correct = y_hat_indices == output\n",
    "       accuracy = np.mean(correct)\n",
    "\n",
    "\n",
    "       # y_true is not one-hot encoded, but instead class indices\n",
    "       loss_value = loss_function.forward(y_hat, y_true=train_set[1][0]).mean()\n",
    "\n",
    "\n",
    "       # dLdY: (b, m, n)\n",
    "       grad_loss = loss_function.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       model.backward(grad_loss)\n",
    "\n",
    "\n",
    "       model.step_gd(optimizer)\n",
    "\n",
    "\n",
    "       print(f'{accuracy=:.5f}, {loss_value=:.5f}')\n",
    "\n",
    "\n",
    "def module_backward_works(input, out_shape: tuple, module):\n",
    "   # Not done, work in progress\n",
    "\n",
    "\n",
    "   # Want dY/dX of this value\n",
    "   grad_output = np.ones(out_shape)\n",
    "\n",
    "\n",
    "   # Compute the forward pass\n",
    "   forward_result = module.forward(input)\n",
    "\n",
    "\n",
    "   # Now do backward with this in mind\n",
    "   dL_dx = module.backward(grad_output)\n",
    "\n",
    "\n",
    "   perturbed = input + delta_input\n",
    "   forward_perturbed = module.forward(perturbed)\n",
    "   # print(forward_perturbed)\n",
    "   print(((forward_perturbed - forward_result).sum() / delta))\n",
    "   # print(grad_output)\n",
    "   # assert np.allclose((forward_perturbed - forward_result) / delta, grad_output, atol=1e-6)\n",
    "\n",
    "\n",
    "def test_backward_correct():\n",
    "   batch_size = 10\n",
    "   in_dims = 2\n",
    "   out_dims = 3\n",
    "   seq_len = 5\n",
    "   module = l.LinearLayer(in_dims, out_dims, has_bias=False)\n",
    "   input = np.random.randn(batch_size, in_dims, seq_len)\n",
    "   module_backward_works(input, (batch_size, out_dims, seq_len), module)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   # test_forward_shape()\n",
    "   # test_backward()\n",
    "   test_adam()\n",
    "   # test_backward_correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "a9f5dfc9faa95b001f0a483cdde4e36b57b705826fc04f26cec8b0bea8b825bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
