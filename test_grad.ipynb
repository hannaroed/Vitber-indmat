{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import jax.random as jrandom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(5, 3, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:3: NumbaPerformanceWarning: '@' is faster on contiguous arrays, called on (Array(float64, 2, 'A', False, aligned=True), Array(float64, 2, 'C', False, aligned=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 1.4901161e-08  0.0000000e+00]]\n",
      "\n",
      " [[ 0.0000000e+00 -2.9802322e-08]\n",
      "  [-2.9802322e-08  2.3841858e-07]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00 -1.1920929e-07]]\n",
      "\n",
      " [[ 5.9604645e-08  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  8.9406967e-08]]]\n",
      "[[[[-1.11205214e-08 -1.54819801e-09]\n",
      "   [-3.27570149e-09 -1.77625523e-08]\n",
      "   [-4.39871535e-08 -2.71883737e-08]]\n",
      "\n",
      "  [[ 1.57698814e-08  2.14102425e-08]\n",
      "   [ 1.61450354e-07 -1.18389391e-07]\n",
      "   [ 5.81359134e-08  6.79754271e-08]]\n",
      "\n",
      "  [[ 1.92657343e-08 -6.84767159e-08]\n",
      "   [ 7.70442909e-09 -1.33064582e-08]\n",
      "   [-3.49659857e-10 -2.50611629e-08]]\n",
      "\n",
      "  [[-1.12520659e-09  3.28681500e-08]\n",
      "   [ 3.17221041e-08  3.73587312e-08]\n",
      "   [ 6.64036603e-09  1.92668210e-08]]\n",
      "\n",
      "  [[ 8.66035483e-08  2.37996582e-08]\n",
      "   [ 1.81227033e-08  7.29825071e-08]\n",
      "   [-1.20852093e-08  2.15488645e-08]]]]\n"
     ]
    }
   ],
   "source": [
    "from layers import LinearLayer\n",
    "import equinox as eqx\n",
    "from layers import batched_mm\n",
    "\n",
    "key = jrandom.PRNGKey(0)\n",
    "\n",
    "linear = LinearLayer(3, 2, True, 0.1)\n",
    "linear.params['w']['w']\n",
    "\n",
    "class Linear(eqx.Module):\n",
    "    weight: jnp.ndarray\n",
    "    bias: jnp.ndarray\n",
    "    def __init__(self, input_dim, output_dim, key):\n",
    "        self.weight = jax.random.normal(key, (output_dim, input_dim))\n",
    "        self.bias = jax.random.normal(key, (output_dim,))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return jnp.einsum('bis,oi->bos', x, self.weight) + self.bias[None, :, None]\n",
    "\n",
    "batch_size, dig, seq = 5, 3, 2\n",
    "x = jrandom.normal(key, (batch_size, 3, seq))\n",
    "jax_linear = Linear(3, 2, jax.random.PRNGKey(0))\n",
    "# out = jax_linear(x)\n",
    "\n",
    "# diffs = jrandom.normal(key, out.shape)\n",
    "\n",
    "# primals, vjp_fun = eqx.filter_vjp(jax_linear.__call__, x)\n",
    "# vjp_fun(out)\n",
    "\n",
    "def test_backward(jax_mod, my_mod, *args):\n",
    "    primals, vjp_fun = eqx.filter_vjp(jax_mod.__call__, *args)\n",
    "    diffs = jrandom.normal(key, primals.shape)\n",
    "    backed = vjp_fun(diffs)\n",
    "\n",
    "    args_mapped = [np.array(x, dtype=np.float64) for x in args]\n",
    "    primals_mine = my_mod.forward(*args_mapped)\n",
    "    backed_mine = my_mod.backward(np.array(diffs, dtype=np.float64))\n",
    "\n",
    "    print(primals - primals_mine)\n",
    "    print(backed - backed_mine)\n",
    "    assert np.allclose(np.array(primals), primals_mine)\n",
    "    assert np.allclose(np.array(backed), backed_mine)\n",
    "\n",
    "linear.params['w']['w'] = np.array(jax_linear.weight, dtype=np.float64).copy()\n",
    "print(linear.params['w']['w'].shape)\n",
    "print(x.shape)\n",
    "# print(batched_mm(linear.params['w']['w'], x).shape)\n",
    "linear.params['b']['w'] = np.array(jax_linear.bias, dtype=np.float64)[:, None].copy()\n",
    "test_backward(jax_linear, linear, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.0000000e+00  2.9802322e-08]\n",
      "  [ 7.4505806e-09  2.9802322e-08]\n",
      "  [ 5.9604645e-08  2.9802322e-08]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-2.9802322e-08  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00 -1.4901161e-08]]\n",
      "\n",
      " [[ 2.9802322e-08  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00]\n",
      "  [ 2.9802322e-08  1.4901161e-08]]]\n",
      "[[[[-1.19512546e-08 -1.15222698e-08]\n",
      "   [-2.23788299e-08 -2.30276542e-08]\n",
      "   [-5.38400856e-08 -1.84939561e-08]]\n",
      "\n",
      "  [[-2.12966785e-08  1.45802842e-09]\n",
      "   [-3.00589783e-08  2.49374237e-09]\n",
      "   [-2.01895425e-09  5.16536428e-09]]\n",
      "\n",
      "  [[-3.36910841e-08 -7.14107423e-09]\n",
      "   [-3.69118398e-08 -1.28443045e-09]\n",
      "   [-2.45850245e-08 -7.82553701e-08]]\n",
      "\n",
      "  [[ 2.08120076e-08 -2.22866270e-09]\n",
      "   [ 2.67632172e-09 -2.59669183e-08]\n",
      "   [ 1.49142169e-08 -1.72183687e-08]]\n",
      "\n",
      "  [[-3.02159220e-09  5.04840421e-09]\n",
      "   [ 8.21870891e-09  3.97482119e-09]\n",
      "   [ 5.50321738e-09 -2.85195861e-09]]]]\n"
     ]
    }
   ],
   "source": [
    "from layers import Softmax\n",
    "class SoftmaxJax(eqx.Module):\n",
    "    def __call__(self, x):\n",
    "        return jax.nn.softmax(x, axis=1)\n",
    "\n",
    "softmax_jax = SoftmaxJax()\n",
    "softmax = Softmax()\n",
    "\n",
    "x = jrandom.normal(key, (batch_size, 3, seq))\n",
    "\n",
    "test_backward(softmax_jax, softmax, x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
