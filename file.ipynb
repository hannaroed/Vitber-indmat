{"cells":[{"cell_type":"markdown","metadata":{},"source":["##### OPPGAVE 1.2\n","\n","\n","Vi skal  nå anta at optimeringen er ferdig og at vi har funnet parameterene ${\\theta}$ som minimerer objektfunksjonen $\\mathcal{L}$, vi skal dermed bruke modellen $f_{\\theta}$ til å predikere $d$ gitt $a, b$ og $c$. Modellen $f_{\\theta}$ skal testes på en sekvens som bare inneholder sifferene til $a, b,$ og $c$ og vi befinner oss i testfasen. I testfasen skal $\\mathbf{x}^{(0)}\\in \\mathbb{Z^{3r}}$, ved å kjøres gjennom modellen, få ut $f_{\\theta}(x) = \\mathbf{z}^{0}$ der $\\mathbf{z}\\in \\mathbb{Z^{3r}}$, altså en sekvens av samme dimensjon.\n","\n","Vi velger fortsatt $r = 2$, $a = 43$, $b = 7$ og $c = 18$ og skal predikere $d$. For å predikere $d\\in \\mathbb{Z^{r+1}}$ skal hver evaluering av modellen predikere neste siffer i sekvensen og legge det til bakerst i $\\mathbf{x}$. Dette blir input i neste prediksjon helt til det er predikert $r+1$ nye heltall som ideelt sett skal være lik $d$. \n","\n","\n","Vi ønsker å finne ${\\theta}$ slik at $\\hat{\\mathbf{y}} = [\\hat{z_5}, \\hat{z_6}, \\hat{z_7}] = [3, 1, 9]= \\mathbf{y}$\n","\n","\n","Vi starter med første sekvens, $\\mathbf{x}^{(0)} = [4,3,0,7,1,8]$, og setter inn i modellen $f_{\\theta}([4,3,0,7,1,8]) = [\\hat{z_0}^{(0)}, \\hat{z_1}^{(0)},..., \\hat{z_5}^{(0)}]$ \n","\n","Legger til bakerste siffer fra outputten inn i neste sekvens: $\\mathbf{x}^{(1)} = [4,3,0,7,1,8, \\hat{z_5}^{(0)}]$. Predikerer neste sekvens: $f_{\\theta}([4,3,0,7,1,8, \\hat{z_5}^{(0)}]) = [\\hat{z_0}^{(1)}, \\hat{z_1}^{(1)},..., \\hat{z_6}^{(1)}]$ \n","\n","Gjentar prosessen: $\\mathbf{x}^{(2)} = [4,3,0,7,1,8, \\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}]$, $f_{\\theta}([4,3,0,7,1,8, \\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}] = [\\hat{z_0}^{(2)}, \\hat{z_1}^{(2)},..., \\hat{z_7}^{(2)}]$\n","\n","Etter $3$ evalueringer av modellen ender vi opp med sekvensen: $\\mathbf{x}^{(3)} = [4,3,0,7,1,8, \\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}, \\hat{z_7}^{(2)}]$\n","\n","Det er nå predikert $r + 1 = 3$ nye heltall, $\\hat{\\mathbf{y}} = [\\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}, \\hat{z_7}^{(2)}]$. Nå kan vi sammenligne den predikerte $\\hat{\\mathbf{y}}$ med vår $\\mathbf{y} = [3, 1, 9]$. Dersom modellen har gjennomgått en god trening skal $\\mathbf{z} = \\hat{\\mathbf{y}} = \\mathbf{y} = [3,1,9]$."]},{"cell_type":"markdown","metadata":{},"source":["##### OPPGAVE 1.3\n","\n","Vi bruker cross-entropy som objektfunksjon, og setter $m = 5$ og $\\mathbf{y} = [4, 3, 2, 1]$.\n","\n","Dersom cross-entropy funksjonen $\\mathcal{L}({\\theta},D) = 0$ betyr det at modellen gir en riktig prediksjon. I et tilfelle der $\\hat{Y} = F_{\\theta}(x) = onehot(\\mathbf{y})$ vil vi få $\\mathcal{L}({\\theta},D) = 0$. Dette betyr også at $\\hat{\\mathbf{y}} = argmax_{col}(\\hat{Y})$\n","\n","Matematisk sett vil vi få: \n","\n","$\\hat{Y} = onehot(\\mathbf{y}) = onehot([4, 3, 2, 1]) := \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\\\end{bmatrix}$\n","\n","\n","Fordelingen til $\\hat{Y}$ tilsvarer en kollonnevis onehot fordeling, der summen av hver kolonne er $1$. For hver kolonne, $i$, vil ideksen av sifferet $1$ representere verdien til tilsvarende siffer, i  $\\mathbf{y_{i}}$. Resten av matrisen $\\hat{Y}$ må derfor bestå av nullere.\n","\n","Slik finner vi videre $\\hat{y}$ ved en kolonnevis $argmax$ operasjon:\n","\n","$\\hat{\\mathbf{y}} = argmax_{col}(\\hat{Y}) = argmax_{col}\\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\\\end{bmatrix} = [4,3,2,1]$\n","\n","Som forventet blir $\\mathbf{\\hat{y}}$ blir lik $\\mathbf{y}$, som alltid vil stemme for denne type fordeling."]},{"cell_type":"markdown","metadata":{},"source":["##### OPPGAVE 1.4\n","Gitt:\n","$\\theta = \\{ W_E, W_P, W_U, \\{W_O^{(l)}, W_V^{(l)}, W_Q^{(l)}, W_K^{(l)}, W_1^{(l)}, W_2^{(l)}\\} _{l=0}^{L-1}\\}$\n","\n","For å kunne bestemme totalt antall enkeltparamtere til transformermodellen, må vi se på dimensjonene til de forskjellige vektmatrisene.\n","Dimensjonen til $W_E$ og $W_U$ er $d \\times m$, dimensjonen til $W_P$ er $d \\times n_{max}$ , dimensjonen til ${W_O}^{(l)}, {W_V}^{(l)}, {W_Q}^{(l)}$ og ${W_K}^{(l)}$ er $k \\times d$ og dimensjonen til ${W_1}^{(l)}$ og ${W_2}^{(l)}$ er $p \\times d$. \n","\n","Vi får da totalt $w = 2dm + dn_{max} + (4kd + 2pd) \\cdot L$ enkeltparametere som vi kan optimere."]},{"cell_type":"markdown","metadata":{},"source":["##### OPPGAVE 1.5\n","\n","Tranformermodellen er beskrevet i likning (4)-(9). Vi skal ta utgangspunkt i disse parameterene:\n","\n","$n = n_{max} = 1$, $m = d = k = p = 2$, $L = 1 \\rightarrow l = 0,..., L-1 = 0$ \n","\n","$W_{O} = W_{V} = W_{Q} = W_{K} = W_{1} = W_{2} = W_{u} = I_{2x2} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\\\\\end{bmatrix} $\n","\n","$\\sigma(x) = \\text{Relu}(x) = \\max(0, x)$\n","\n","$ W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix}$, $ W_{p} = \\begin{bmatrix} 1 \\\\ 0\\\\\\end{bmatrix}$\n","\n","$D\\in{\\mathbb{R}^{n\\times n}}$ er null på øvre triangulær inkludert diagonalen, slik at $n = 1$ gir $D = [0]$.\n","\n","Videre vil vi vise at vi må ha $\\alpha > 1$ for å få $\\hat{z} = [1]$ som output når input er $x = [1]$.\n","\n","Vi begynner med likning $(4)$ $X = onehot(\\mathbf{x})$. Her tar vi inn en vektor $\\mathbf{x}\\in{\\mathbb{Z}^{1}}$ og får ut en matrise $X$.\n","\n","\\begin{equation} \n","X = \\text{onehot}([1]) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad X \\in \\mathbb{Z}^{2\\times1} \\tag{4}\n","\\end{equation}\n","\n","\n","I likning $(5)$  $z_{0} = W_{E}X + [W_{p}]_{0:n}$ tar vi inn matrisen $X$ fra likning $(4)$ og får ut en matrise $z_{0}$.\n","\n","\\begin{equation} \n","z_{0} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\\\\\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1\\\\\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0\\\\\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\alpha\\\\\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1\\\\\\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\alpha\\\\\\end{bmatrix}, \\quad z_{0}\\in{\\mathbb{R}^{2 \\times 1}} \\tag{5}\n","\\end{equation}\n","\n","I likning $(6)$  $z_{\\frac{1}{2}} = f_{0}^{A}(z_{0}) = z_{0} + W_{0}^{T} W_{V} z_{0} A(z_{0})$ får vi ut en matrise $z_{\\frac{1}{2}}$.\n","\n","For å finne denne må vi første finne $softmax_{col}$ av $z_{0}$. Det gjør vi i likning $(3)$ $A(z_{0}) = softmax_{col}(z_{0}^{T} W_{Q}^{T} W_{k} z_{0} + D)$.\n","\n","\\begin{equation}\n","A(\\begin{bmatrix} 1 \\\\ \\alpha \\\\\\end{bmatrix}) = softmax_{col}(\\begin{bmatrix} 1 & \\alpha \\\\\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\alpha\\\\\\end{bmatrix} + [0]) = softmax_{col}(\\begin{bmatrix} 1 & \\alpha \\\\\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\alpha \\\\\\end{bmatrix} + [0]) = softmax_{col}([\\alpha^{2} + 1]) = \\frac{\\exp^{1 + \\alpha^2}}{\\exp^{1 + \\alpha^2}} = 1 \\tag{3}\n","\\end{equation}\n","\n","Bruker så dette i likning (6):\n","\n","\\begin{equation}\n","z_{\\frac{1}{2}} = f_{0}^{A}(z_{0}) = \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} + \\begin{bmatrix} 1&0\\\\0&1\\\\\\end{bmatrix} \\begin{bmatrix} 1&0\\\\0&1\\\\\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} A(\\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix}) = \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\alpha \\\\ \\end{bmatrix},\\quad z_{\\frac{1}{2}}\\in{\\mathbb{R}^{2 \\times 1}} \\tag{6}\n","\\end{equation}\n","\n","\n","I likning $(7)$  $z_{1} = f_{0}^{F}(z_{\\frac{1}{2}}) = z_{\\frac{1}{2}} + W_{2}^{T} \\sigma(W_{1}z_{\\frac{1}{2}})$ skal vi få ut en matrise $z_{1}$.\n","\n","\\begin{equation}\n","z_{1} = f_{0}^{F}(z_{\\frac{1}{2}}) = \\begin{bmatrix} 2 \\\\ 2 \\alpha \\\\\\end{bmatrix} + \\begin{bmatrix} 1&0\\\\0&1\\\\\\end{bmatrix} \\sigma (\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + max(0,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}), \\quad z_{1}\\in{\\mathbb{R}^{2\\times1}} \\tag{7}\n","\\end{equation}\n","\n","Her får vi to utfall:\n","\n","\\begin{equation}\n","For\\, \\alpha > 0:\\, \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + max(0,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} = \\begin{bmatrix}4\\\\4\\alpha\\\\\\end{bmatrix},\\quad z_{1}\\in{\\mathbb{R}^{2\\times1}} \\tag{7.1}\n","\\end{equation}\n","\n","\\begin{equation}\n","For\\, \\alpha < 0:\\,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + max(0,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + \\begin{bmatrix}2\\\\0\\\\\\end{bmatrix} = \\begin{bmatrix}4\\\\2\\alpha\\\\\\end{bmatrix},\\quad z_{1}\\in{\\mathbb{R}^{2\\times1}} \\tag{7.2}\n","\\end{equation}\n","\n","\n","I likning $(8)$  $Z = softmax_{col}(W_{U}^{T}z_{L})$ får vi ut en matrise.\n","\n","\\begin{equation}\n","For \\, \\alpha > 0: softmax_{col}(\\begin{bmatrix}4\\\\4\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}  \\frac{e^{4}}{e^{4} + e^{4\\alpha}} \\\\  \\frac{e^{4\\alpha}}{e^{4} + e^{4\\alpha}} \\\\\\end{bmatrix}, \\quad Z\\in{\\mathbb{R}^{2\\times1}} \\tag{8.1}\n","\\end{equation}\n","\n","\\begin{equation}\n","For \\, \\alpha < 0: softmax_{col}(\\begin{bmatrix}4\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix} \\frac{e^{4}}{e^{4} + e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^{4} + e^{2\\alpha}} \\\\\\end{bmatrix}, \\quad Z\\in{\\mathbb{R}^{2\\times1}} \\tag{8.2}\n","\\end{equation}\n","\n","\n","I likning $(9)$ $\\hat{z} = argmax_{col}(Z)$ får vi ut et matrise.\n","\n","\\begin{equation}\n","For \\, \\alpha > 0: argmax_{col}(\\begin{bmatrix}  \\frac{e^{4}}{e^{4} + e^{4\\alpha}} \\\\  \\frac{e^{4\\alpha}}{e^{4} + e^{4\\alpha}} \\\\\\end{bmatrix}), \\quad \\hat{z}\\in\\mathbb{Z}^{1} \\tag{9.1}\n","\\end{equation}\n","\n","\\begin{equation}\n","For \\, \\alpha < 0: argmax_{col}(\\begin{bmatrix}  \\frac{e^{4}}{e^{4} + e^{2\\alpha}} \\\\  \\frac{e^{2\\alpha}}{e^{4} + e^{2\\alpha}} \\\\\\end{bmatrix}), \\quad \\hat{z}\\in\\mathbb{Z}^{1} \\tag{9.2}\n","\\end{equation}\n","\n","For at $\\hat{z} = 1$ må vi ha at det nederste elementet i vektoren være størst, altså:\n","\n","I likning $(8.1)$ får vi dermed $\\frac{e^{4\\alpha}}{e^{4} + e^{4\\alpha}} > \\frac{e^{4}}{e^{4} + e^{4\\alpha}}$ $\\rightarrow$ $\\alpha > 1$\n","\n","Tilsvarende for likning $(8.2):$ $\\frac{e^{2\\alpha}}{e^{4} + e^{2\\alpha}} > \\frac{e^{4}}{e^{4} + e^{2\\alpha}}$ $\\rightarrow$ $\\alpha > 2$\n","\n","I likning $(8.2)$ får vi både $\\alpha < 0$, $\\alpha > 2$, og dette gir $L = \\emptyset$.\n","Vi har dermed eneste gyldige løsning $\\alpha > 1$ for at $\\hat{z} = 1. \\quad \\blacksquare$"]},{"cell_type":"markdown","metadata":{},"source":["#### OPPGAVE 2"]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 2.1\n","Transformermodellen vår består av et nevralt nettverk, som er implementert i neural_network.py. Vi har brukt objektorientert programmering for å implementere alle lagene som bygger opp nettverket. Baseklassen Layer inneholder forward(), backward() og step_gd() som er funksjonene til lagene. I Neural_network settes alle lagene sammen og det kjøres forward pass, backward pass og gradient descent på alle lagene. \n","\n","Grunnen til at vi bruker objektorientert programmering er at lagene skal kunne arve fra hverandre. I tillegg vil dette gi en ryddig og strukturert kode. Klassen Layer er baseklassen, med flere underklasser. Alle underklassene arver funksjonene fra Layer, underklassene overskriver forward() og backward() funksjonene fra Layer slik at de er tilpasset det spesifikke laget. step_gd() er en optimeringsalgoritme og denne vil arves av resten av lagene. (Det er kun de lagene med parametere), altså lag som ikke inneholder underlag, som beholder step_gd() som definert i Layer. Resten av lagene inneholder underlag og overskriver step_gd() slik at den kan utfører gradient sescent på hvert av underlagene.\n","\n","Vårt nevrale nettverk er bygget opp av lagene EmbedPosition, TransformerBlock, LinearLayer og Softmax, som igjen er bygget opp av flere underliggende lag. NeuralNetwork bruker arv når den utfører gradient descent på lagene ved at de enten arver fra et av de overliggende lagene, eller overskriver step_gd()."]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 2.2\n","\n","I vår implementering av transformermodellen har vi valgt en annen struktur enn utdelt kode, der vi har implementert flere lag. Vi skal nå forklare hvordan disse er satt sammen.\n","\n","Vi har delt Attention opp i flere lag, der SelfAttention er øverst. Hovedoppgaven til SelfAttention er at den skal gjøre det mulig for det nevrale nettverket å fokusere på spesifikke deler av den innhentede dataen. Dette gjør det enklere å håndtere lange sekvenser av data og (fokusere på relevante deler av dataen) identifisere viktige mønstre. \n","\n","Attention er undeklassen til SelfAttention, i forward passet tar vi inn ‘queries, ‘keys’ og ‘values’, disse tre jobber sammen for å bestemme hvor mye vekt eller attention som skal tilegnes forskjellige deler av inputen. Den returnerer vektmatrisen ganget med attention,\n","I backward kalkulerer vi loss gradienten med hensyn på en rekke variabler.\n","\n","Inni Attention blir det benyttet et annet underlag, Softmax. Dette laget utfører kollonevis softmax operasjon, som tar inn en tre dimensjonal matrise $x$ skalerer verdiene i matrisen til en kolonnevis sannsynlighetsfordeling. Backward vil returnere gradienten til lossen med hensyn på $z_l$ som er hentet fra forward passet. \n","\n","I SelfAttention brukes både Attention og Softmax. Derfor overskrives step_gd() fra baseklassen Layers fordi vi ønsker å utføre en iterasjon av gradient descent på alle vektmatrisene.\n","\n","FeedForward:\n","\n","I TransformerBlock laget settes SelfAttantion og FeedForward lagene sammen, \n"," \n","Unembedding er et lineært lag med dimensjon (d,m).\n","\n","Relu er aktiveringsfunksjonen vi tar i bruk. Denne er viktig fordi den introduserer ikke-linearitet i det ellers lineære systemet vårt. Uten denne ville vi ikke kunne fått fram komplekse relasjoner i dataene, ettersom at en serie med lineære operasjoner kan bli redusert til èn enkelt lineær operasjon. Forward i Relu tar simpelt inn en matrise og returnerer samme dimensjon bare at alle element som var negative før er nå null. Backward tar inn gradienten og returnerer den multiplisert med den deriverte av Relu.  "]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 2.3\n","Adam er en optimeringsalgoritme som er raskere enn gradient descent. Vi har implementert Adam ved å lage en ny foreldreklasse Optimizer og en barneklasse Adam. Dette gjør koden vår mer generell som gjør det mulig å lage flere optimaliseringsalgoritmer som kan brukes i nettverket. I Adam er det definert to nye variabler M, momentum, og V som gir informasjon om tidligere deriverte, dette skal forbedre algoritmen. Vi har derfor endret step_gd() til å ta inn Optimizer som en parameter. Parameterne blir lagt til i den nøstede dictionaryen hver gang dataen går gjennom et underlag ved step_gd(). \n","\n","\n","Numba\n","I prosjektoppgaven vår har flere større beregninger og jo større nettverket er jo langer kjøretid får vi. For å minimere kjøretiden har vi kun brukt numpy, og har derav endret en del i koden som var laget fra før. Dette er gjort slik at vi koden skal kunne kompilleres ved hjelp av jit og njit (‘A ~5 minute guide to Numba’). I objektorientert programmering ved numba må vi på forhånd si hvilke variabler som skal bli definert inni klassen, så før hver klasse har vi laget en @jitclass med nye variabler og deres type.  \n"]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 3.1"]},{"cell_type":"markdown","metadata":{},"source":["#### Oppgave 3.2\n","Printer gjennomsnittet av objektfunskjonen(losset) over batchene :\n","$\\mathcal{L}^{j} = \\frac{1}{B}  \\sum_{k=0}^{B-1}\\mathcal{L}^{j}_{k}$\n","\n","Losset skal bli mindre for hver iterasjon"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n = 100\n","\n","test_model_1 = make_model(r=5, d=10, m=2, L=2, p=15, k=5)\n","test_model_2 = make_model(r=7, d=20, m=5, L=2, p=25, k=10)\n","\n","sorting_data1 = get_train_test_sorting(5, 2, 250, 10, 10)\n","trained_model1, mean_loss_arr1 = training_sorting(test_model_1, CrossEntropy(), Adam(), sorting_data1, 2, n_epochs=n, r=5)\n","\n","sorting_data2 = get_train_test_sorting(7, 5, 250, 20, 20)\n","trained_model2, mean_loss_arr2 = training_sorting(test_model_2, CrossEntropy(), Adam(), sorting_data2, 5, n_epochs=n, r=7)\n","\n","iter = np.arange(n)\n","\n","plt.title(\"Modell 1\")\n","plt.plot(iter, mean_loss_arr1)\n","plt.show()\n","plt.title(\"Modell 2\")\n","plt.plot(iter, mean_loss_arr2)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy1 = test_sorting(trained_model1, sorting_data1, m=2)\n","print(f'Accuracy for model 1: {np.round(accuracy1, 1)}%')\n","\n","accuracy2 = test_sorting(trained_model2, sorting_data2, m=5)\n","print(f'Accuracy for model 2: {np.round(accuracy2, 1)}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n = 100\n","\n","test_model_add = make_model(r=6, d=30, m=10, L=3, p=40, k=20)\n","addition_data = get_train_test_addition(n_digit=2, samples_per_batch=250, n_batches_train=20, n_batches_test=20)\n","\n","trained_model_add, mean_loss_arr_add = training_addition(test_model_add, CrossEntropy(), Adam(), addition_data, 10, n_epochs=n, r=2)\n","\n","iter = np.arange(n)\n","\n","plt.title(\"Addition loss\")\n","plt.plot(iter, mean_loss_arr_add)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_addition = test_addition(trained_model_add, addition_data, m=10)\n","print(f'Accuracy for model: {np.round(accuracy_addition, 1)}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.12.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
