{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Transformermodellen for prediksjon av sekvenser"]},{"cell_type":"markdown","metadata":{},"source":["#### OPPGAVE 1"]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 1.1\n","\n","For å trene en transformermodell så trenger vi et datasett som består av flere x og y vektorer, $ D = \\{ (\\mathbf{x_0}, \\mathbf{y_0}), (\\mathbf{x_1}, \\mathbf{y_1}), \\ldots, (\\mathbf{x_{n-1}}, \\mathbf{y_{n-1}}) \\} $. For å vise et eksempel på et av elementene i datasettet, $(\\mathbf{x_0}, \\mathbf{y_0})$, velger vi $r = 2$, $a = 43$, $b = 7$, $c = 18$ og $d = a \\cdot b + c = 319$. Vektoren $\\mathbf{x_0}$ er bygget opp slik at vi får de $r$ første sifferene av $a$, $b$, $c$ og $d$ og lengden til $\\mathbf{x_i}$ blir dermed $4 \\cdot r$. I dette tilfellet får vi da $\\mathbf{x_0} = [4, 3, 0, 7, 1, 8, 3, 1]$. $\\mathbf{y_0}$ består av alle sifferene til $d$ og blir her $\\mathbf{y_0} = [3, 1, 9]$.  "]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 1.2\n","\n","\n","Vi skal  nå anta at optimeringen er ferdig og at vi har funnet parameterene ${\\theta}$ som minimerer objektfunksjonen $\\mathcal{L}$, vi skal dermed bruke modellen $f_{\\theta}$ til å predikere $d$ gitt $a, b$ og $c$. Modellen $f_{\\theta}$ skal testes på en sekvens som bare inneholder sifferene til $a, b,$ og $c$ og vi befinner oss i testfasen. I testfasen skal $\\mathbf{x}^{(0)}\\in \\mathbb{Z^{3r}}$, ved å kjøres gjennom modellen, få ut $f_{\\theta}(x) = \\mathbf{z}^{0}$ der $\\mathbf{z}\\in \\mathbb{Z^{3r}}$, altså en sekvens av samme dimensjon.\n","\n","Vi velger fortsatt $r = 2$, $a = 43$, $b = 7$ og $c = 18$ og skal predikere $d$. For å predikere $d\\in \\mathbb{Z^{r+1}}$ skal hver evaluering av modellen predikere neste siffer i sekvensen og legge det til bakerst i $\\mathbf{x}$. Dette blir input i neste prediksjon helt til det er predikert $r+1$ nye heltall som ideelt sett skal være lik $d$. \n","\n","\n","Vi ønsker å finne ${\\theta}$ slik at $\\hat{\\mathbf{y}} = [\\hat{z_5}, \\hat{z_6}, \\hat{z_7}] = [3, 1, 9]= \\mathbf{y}$\n","\n","\n","Vi starter med første sekvens, $\\mathbf{x}^{(0)} = [4,3,0,7,1,8]$, og setter inn i modellen $f_{\\theta}([4,3,0,7,1,8]) = [\\hat{z_0}^{(0)}, \\hat{z_1}^{(0)},..., \\hat{z_5}^{(0)}]$ \n","\n","Legger til bakerste siffer fra outputten inn i neste sekvens: $\\mathbf{x}^{(1)} = [4,3,0,7,1,8, \\hat{z_5}^{(0)}]$. Predikerer neste sekvens: $f_{\\theta}([4,3,0,7,1,8, \\hat{z_5}^{(0)}]) = [\\hat{z_0}^{(1)}, \\hat{z_1}^{(1)},..., \\hat{z_6}^{(1)}]$ \n","\n","Gjentar prosessen: $\\mathbf{x}^{(2)} = [4,3,0,7,1,8, \\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}]$, $f_{\\theta}([4,3,0,7,1,8, \\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}] = [\\hat{z_0}^{(2)}, \\hat{z_1}^{(2)},..., \\hat{z_7}^{(2)}]$\n","\n","Etter $3$ evalueringer av modellen ender vi opp med sekvensen: $\\mathbf{x}^{(3)} = [4,3,0,7,1,8, \\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}, \\hat{z_7}^{(2)}]$\n","\n","Det er nå predikert $r + 1 = 3$ nye heltall, $\\hat{\\mathbf{y}} = [\\hat{z_5}^{(0)}, \\hat{z_6}^{(1)}, \\hat{z_7}^{(2)}]$. Nå kan vi sammenligne den predikerte $\\hat{\\mathbf{y}}$ med vår $\\mathbf{y} = [3, 1, 9]$. Dersom modellen har gjennomgått en god trening skal $\\mathbf{z} = \\hat{\\mathbf{y}} = \\mathbf{y} = [3,1,9]$."]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 1.3\n","\n","Vi bruker cross-entropy som objektfunksjon, og setter $m = 5$ og $\\mathbf{y} = [4, 3, 2, 1]$.\n","\n","Dersom cross-entropy funksjonen $\\mathcal{L}({\\theta},D) = 0$ betyr det at modellen gir en riktig prediksjon. I et tilfelle der $\\hat{Y} = F_{\\theta}(x) = onehot(\\mathbf{y})$ vil vi få $\\mathcal{L}({\\theta},D) = 0$. Dette betyr også at $\\hat{\\mathbf{y}} = argmax_{col}(\\hat{Y})$\n","\n","Matematisk sett vil vi få: \n","\n","$\\hat{Y} = onehot(\\mathbf{y}) = onehot([4, 3, 2, 1]) := \\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\\\end{bmatrix}$\n","\n","\n","Fordelingen til $\\hat{Y}$ tilsvarer en kollonnevis onehot fordeling, der summen av hver kolonne er $1$. For hver kolonne, $i$, vil ideksen av sifferet $1$ representere verdien til tilsvarende siffer, i  $\\mathbf{y_{i}}$. Resten av matrisen $\\hat{Y}$ må derfor bestå av nullere.\n","\n","Slik finner vi videre $\\hat{y}$ ved en kolonnevis $argmax$ operasjon:\n","\n","$\\hat{\\mathbf{y}} = argmax_{col}(\\hat{Y}) = argmax_{col}\\begin{bmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\\\end{bmatrix} = [4,3,2,1]$\n","\n","Som forventet blir $\\mathbf{\\hat{y}}$ blir lik $\\mathbf{y}$, som alltid vil stemme for denne type fordeling."]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 1.4\n","Gitt:\n","$\\theta = \\{ W_E, W_P, W_U, \\{W_O^{(l)}, W_V^{(l)}, W_Q^{(l)}, W_K^{(l)}, W_1^{(l)}, W_2^{(l)}\\} _{l=0}^{L-1}\\}$\n","\n","For å kunne bestemme totalt antall enkeltparamtere til transformermodellen, må vi se på dimensjonene til de forskjellige vektmatrisene.\n","Dimensjonen til $W_E$ og $W_U$ er $d \\times m$, dimensjonen til $W_P$ er $d \\times n_{max}$ , dimensjonen til ${W_O}^{(l)}, {W_V}^{(l)}, {W_Q}^{(l)}$ og ${W_K}^{(l)}$ er $k \\times d$ og dimensjonen til ${W_1}^{(l)}$ og ${W_2}^{(l)}$ er $p \\times d$. \n","\n","Vi får da totalt $w = 2dm + dn_{max} + (4kd + 2pd) \\cdot L$ enkeltparametere som vi kan optimere."]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 1.5\n","\n","Tranformermodellen er beskrevet i likning (4)-(9). Vi skal ta utgangspunkt i disse parameterene:\n","\n","$n = n_{max} = 1$, $m = d = k = p = 2$, $L = 1 \\rightarrow l = 0,..., L-1 = 0$ \n","\n","$W_{O} = W_{V} = W_{Q} = W_{K} = W_{1} = W_{2} = W_{u} = I_{2x2} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1\\\\\\end{bmatrix} $\n","\n","$\\sigma(x) = \\text{Relu}(x) = \\max(0, x)$\n","\n","$ W_{E} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\end{bmatrix}$, $ W_{p} = \\begin{bmatrix} 1 \\\\ 0\\\\\\end{bmatrix}$\n","\n","$D\\in{\\mathbb{R}^{n\\times n}}$ er null på øvre triangulær inkludert diagonalen, slik at $n = 1$ gir $D = [0]$.\n","\n","Videre vil vi vise at vi må ha $\\alpha > 1$ for å få $\\hat{z} = [1]$ som output når input er $x = [1]$.\n","\n","Vi begynner med likning $(4)$ $X = onehot(\\mathbf{x})$. Her tar vi inn en vektor $\\mathbf{x}\\in{\\mathbb{Z}^{1}}$ og får ut en matrise $X$.\n","\n","\\begin{equation} \n","X = \\text{onehot}([1]) = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad X \\in \\mathbb{Z}^{2\\times1} \\tag{4}\n","\\end{equation}\n","\n","\n","I likning $(5)$  $z_{0} = W_{E}X + [W_{p}]_{0:n}$ tar vi inn matrisen $X$ fra likning $(4)$ og får ut en matrise $z_{0}$.\n","\n","\\begin{equation} \n","z_{0} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\alpha \\\\\\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 1\\\\\\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0\\\\\\end{bmatrix} = \\begin{bmatrix} 0 \\\\ \\alpha\\\\\\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1\\\\\\end{bmatrix} = \\begin{bmatrix} 1 \\\\ \\alpha\\\\\\end{bmatrix}, \\quad z_{0}\\in{\\mathbb{R}^{2 \\times 1}} \\tag{5}\n","\\end{equation}\n","\n","I likning $(6)$  $z_{\\frac{1}{2}} = f_{0}^{A}(z_{0}) = z_{0} + W_{0}^{T} W_{V} z_{0} A(z_{0})$ får vi ut en matrise $z_{\\frac{1}{2}}$.\n","\n","For å finne denne må vi første finne $softmax_{col}$ av $z_{0}$. Det gjør vi i likning $(3)$ $A(z_{0}) = softmax_{col}(z_{0}^{T} W_{Q}^{T} W_{k} z_{0} + D)$.\n","\n","\\begin{equation}\n","A(\\begin{bmatrix} 1 \\\\ \\alpha \\\\\\end{bmatrix}) = softmax_{col}(\\begin{bmatrix} 1 & \\alpha \\\\\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\\\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\alpha\\\\\\end{bmatrix} + [0]) = softmax_{col}(\\begin{bmatrix} 1 & \\alpha \\\\\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\alpha \\\\\\end{bmatrix} + [0]) = softmax_{col}([\\alpha^{2} + 1]) = \\frac{\\exp^{1 + \\alpha^2}}{\\exp^{1 + \\alpha^2}} = 1 \\tag{3}\n","\\end{equation}\n","\n","Bruker så dette i likning (6):\n","\n","\\begin{equation}\n","z_{\\frac{1}{2}} = f_{0}^{A}(z_{0}) = \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} + \\begin{bmatrix} 1&0\\\\0&1\\\\\\end{bmatrix} \\begin{bmatrix} 1&0\\\\0&1\\\\\\end{bmatrix} \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} A(\\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix}) = \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ \\alpha \\\\ \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\alpha \\\\ \\end{bmatrix},\\quad z_{\\frac{1}{2}}\\in{\\mathbb{R}^{2 \\times 1}} \\tag{6}\n","\\end{equation}\n","\n","\n","I likning $(7)$  $z_{1} = f_{0}^{F}(z_{\\frac{1}{2}}) = z_{\\frac{1}{2}} + W_{2}^{T} \\sigma(W_{1}z_{\\frac{1}{2}})$ skal vi få ut en matrise $z_{1}$.\n","\n","\\begin{equation}\n","z_{1} = f_{0}^{F}(z_{\\frac{1}{2}}) = \\begin{bmatrix} 2 \\\\ 2 \\alpha \\\\\\end{bmatrix} + \\begin{bmatrix} 1&0\\\\0&1\\\\\\end{bmatrix} \\sigma (\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + max(0,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}), \\quad z_{1}\\in{\\mathbb{R}^{2\\times1}} \\tag{7}\n","\\end{equation}\n","\n","Her får vi to utfall:\n","\n","\\begin{equation}\n","For\\, \\alpha > 0:\\, \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + max(0,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} = \\begin{bmatrix}4\\\\4\\alpha\\\\\\end{bmatrix},\\quad z_{1}\\in{\\mathbb{R}^{2\\times1}} \\tag{7.1}\n","\\end{equation}\n","\n","\\begin{equation}\n","For\\, \\alpha < 0:\\,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + max(0,\\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}2\\\\2\\alpha\\\\\\end{bmatrix} + \\begin{bmatrix}2\\\\0\\\\\\end{bmatrix} = \\begin{bmatrix}4\\\\2\\alpha\\\\\\end{bmatrix},\\quad z_{1}\\in{\\mathbb{R}^{2\\times1}} \\tag{7.2}\n","\\end{equation}\n","\n","\n","I likning $(8)$  $Z = softmax_{col}(W_{U}^{T}z_{L})$ får vi ut en matrise.\n","\n","\\begin{equation}\n","For \\, \\alpha > 0: softmax_{col}(\\begin{bmatrix}4\\\\4\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix}  \\frac{e^{4}}{e^{4} + e^{4\\alpha}} \\\\  \\frac{e^{4\\alpha}}{e^{4} + e^{4\\alpha}} \\\\\\end{bmatrix}, \\quad Z\\in{\\mathbb{R}^{2\\times1}} \\tag{8.1}\n","\\end{equation}\n","\n","\\begin{equation}\n","For \\, \\alpha < 0: softmax_{col}(\\begin{bmatrix}4\\\\2\\alpha\\\\\\end{bmatrix}) = \\begin{bmatrix} \\frac{e^{4}}{e^{4} + e^{2\\alpha}} \\\\ \\frac{e^{2\\alpha}}{e^{4} + e^{2\\alpha}} \\\\\\end{bmatrix}, \\quad Z\\in{\\mathbb{R}^{2\\times1}} \\tag{8.2}\n","\\end{equation}\n","\n","\n","I likning $(9)$ $\\hat{z} = argmax_{col}(Z)$ får vi ut et matrise.\n","\n","\\begin{equation}\n","For \\, \\alpha > 0: argmax_{col}(\\begin{bmatrix}  \\frac{e^{4}}{e^{4} + e^{4\\alpha}} \\\\  \\frac{e^{4\\alpha}}{e^{4} + e^{4\\alpha}} \\\\\\end{bmatrix}), \\quad \\hat{z}\\in\\mathbb{Z}^{1} \\tag{9.1}\n","\\end{equation}\n","\n","\\begin{equation}\n","For \\, \\alpha < 0: argmax_{col}(\\begin{bmatrix}  \\frac{e^{4}}{e^{4} + e^{2\\alpha}} \\\\  \\frac{e^{2\\alpha}}{e^{4} + e^{2\\alpha}} \\\\\\end{bmatrix}), \\quad \\hat{z}\\in\\mathbb{Z}^{1} \\tag{9.2}\n","\\end{equation}\n","\n","For at $\\hat{z} = 1$ må vi ha at det nederste elementet i vektoren være størst, altså:\n","\n","I likning $(8.1)$ får vi dermed $\\frac{e^{4\\alpha}}{e^{4} + e^{4\\alpha}} > \\frac{e^{4}}{e^{4} + e^{4\\alpha}}$ $\\rightarrow$ $\\alpha > 1$\n","\n","Tilsvarende for likning $(8.2):$ $\\frac{e^{2\\alpha}}{e^{4} + e^{2\\alpha}} > \\frac{e^{4}}{e^{4} + e^{2\\alpha}}$ $\\rightarrow$ $\\alpha > 2$\n","\n","I likning $(8.2)$ får vi både $\\alpha < 0$, $\\alpha > 2$, og dette gir $L = \\emptyset$.\n","Vi har dermed eneste gyldige løsning $\\alpha > 1$ for at $\\hat{z} = 1. \\quad \\blacksquare$"]},{"cell_type":"markdown","metadata":{},"source":["#### OPPGAVE 2"]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 2.1\n","Transformermodellen vår består av et nevralt nettverk, som er implementert i neural_network.py. Vi har brukt objektorientert programmering for å implementere alle lagene som bygger opp nettverket. Baseklassen Layer inneholder forward(), backward() og step_gd() som er funksjonene til lagene. I Neural_network settes alle lagene sammen og det kjøres forward pass, backward pass og gradient descent på alle lagene. \n","\n","Grunnen til at vi bruker objektorientert programmering er at lagene skal kunne arve fra hverandre. I tillegg vil dette gi en ryddig og strukturert kode. Klassen Layer er baseklassen, med flere underklasser. Alle underklassene arver funksjonene fra Layer, underklassene overskriver forward() og backward() funksjonene fra Layer slik at de er tilpasset det spesifikke laget. step_gd() er en optimeringsalgoritme og denne vil arves av resten av lagene. Det er kun de lagene med parametere, altså lag som ikke inneholder underlag, som beholder step_gd() som definert i Layer. Resten av lagene inneholder underlag og overskriver step_gd() slik at den kan utfører gradient sescent på hvert av underlagene.\n","\n","Vårt nevrale nettverk er bygget opp av lagene EmbedPosition, TransformerBlock, LinearLayer og Softmax, som igjen er bygget opp av flere underliggende lag. NeuralNetwork bruker arv når den utfører gradient descent på lagene ved at de enten arver fra et av de overliggende lagene, eller overskriver step_gd()."]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 2.2\n","\n","I vår implementering av transformermodellen har vi valgt å endre på deler av den gitte koden. Dette har vi gjort for at koden skal kunne kompilleres ved hjelp av numba. Oppgaven består av større beregninger og defor ønsket vi å minimere kjøretiden ved hjelp av jit og njit (‘A ~5 minute guide to Numba’). Vi har altså ikke endret hva den gitte koden gjør, bare gjort den raskere.\n","\n","Vi har også valgt å strukturere koden vår noe annerledes enn utdelt kode, der vi har implementert flere lag. Videre skal vi nå forklare hvordan vi har valgt å strukturere lagene våres.\n","\n","En av endringene vi har valgt å gjøre i er at vi har laget en ny klasse Transformerblock. Denne er en sammensetning av underlagene SelfAttention og FeedForward. \n","\n","Vi har også delt Attention opp i flere lag, der SelfAttention er øverst. Hovedoppgaven til SelfAttention er at den skal gjøre det mulig for det nevrale nettverket å fokusere på spesifikke deler av den innhentede dataen. Dette gjør det enklere å håndtere lange sekvenser av data og (fokusere på relevante deler av dataen) identifisere viktige mønstre. \n","\n","Attention er undeklassen til SelfAttention, i forward passet tar vi inn ‘queries, ‘keys’ og ‘values’, disse tre jobber sammen for å bestemme hvor mye vekt eller attention som skal tilegnes forskjellige deler av inputen. Den returnerer vektmatrisen ganget med attention,\n","I backward kalkulerer vi loss gradienten med hensyn på en rekke variabler.\n","\n","I tillegg har vi valgt å legge til bias som en ekstra parameter i den nøstede dictionaryen, da dette vil forbedre losset vårt og bedre accuracyen. KOM MED KILDE HER OG FORKLAR BEDRE"]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 2.3\n","Adam er en optimeringsalgoritme som er raskere enn gradient descent. Vi har implementert Adam ved å lage en ny foreldreklasse Optimizer og en barneklasse Adam. Dette gjør koden vår mer generell som gjør det mulig å lage flere optimaliseringsalgoritmer som kan brukes i nettverket. I Adam er det definert to nye variabler M, momentum, og V som gir informasjon om tidligere deriverte, dette skal forbedre algoritmen. Vi har derfor endret step_gd() til å ta inn Optimizer som en parameter. Parameterne blir lagt til i den nøstede dictionaryen hver gang dataen går gjennom et underlag ved step_gd(). "]},{"cell_type":"markdown","metadata":{},"source":["#### OPPGAVE 3"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"]}],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import os\n","# os.environ['NUMBA_DISABLE_JIT'] = '1'\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import importlib\n","import training\n","\n","from training import make_model, training_sorting\n","from layers import CrossEntropy\n","from optimizer import Adam\n","from data_generators import get_train_test_sorting, get_train_test_addition\n","\n","from training import make_model, training_addition\n","from testing import test_sorting\n","import importlib\n","import testing\n","from testing import test_addition"]},{"cell_type":"markdown","metadata":{},"source":["##### Oppgave 3.1"]},{"cell_type":"markdown","metadata":{},"source":["#### Oppgave 3.2\n","Printer gjennomsnittet av objektfunskjonen(losset) over batchene :\n","$\\mathcal{L}^{j} = \\frac{1}{B}  \\sum_{k=0}^{B-1}\\mathcal{L}^{j}_{k}$\n","\n","Losset skal bli mindre for hver iterasjon"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/var/folders/zw/n3p4mhqx5kz4l8y4562dkjsm0000gn/T/ipykernel_26962/4117542254.py\", line 3, in <module>\n","    test_model_1 = make_model(r=5, d=10, m=2, L=2, p=15, k=5)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/Documents/Dokumenter – Camillas MacBook Air/vitber/Vitber-indmat-1/training.py\", line 31, in make_model\n","    model = NeuralNetwork(r, d, m, L, p, k)\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/numba/experimental/jitclass/base.py\", line 124, in __call__\n","    return cls._ctor(*bind.args[1:], **bind.kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/numba/core/dispatcher.py\", line 468, in _compile_for_args\n","    error_rewrite(e, 'typing')\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/numba/core/dispatcher.py\", line 409, in error_rewrite\n","    raise e.with_traceback(None)\n","numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n","Failed in nopython mode pipeline (step: nopython frontend)\n","Failed in nopython mode pipeline (step: native lowering)\n","Cannot cast instance.jitclass.LinearLayer#1151d3020<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool> to instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>: %\".58\" = load {i8*, {{i8*, i8*}, {i8*, i8*, i64, i64, double*, [3 x i64], [3 x i64]}, i8}*}, {i8*, {{i8*, i8*}, {i8*, i8*, i64, i64, double*, [3 x i64], [3 x i64]}, i8}*}* %\".25\"\n","During: lowering \"(self).embed = $22call.6\" at /Users/camillaandas/Documents/Dokumenter – Camillas MacBook Air/vitber/Vitber-indmat-1/layers.py (475)\n","During: resolving callee type: jitclass.EmbedPosition#123e539b0<embed:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,w:array(float64, 2d, A),params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>>\n","During: typing of call at /Users/camillaandas/Documents/Dokumenter – Camillas MacBook Air/vitber/Vitber-indmat-1/neural_network.py (22)\n","\n","During: resolving callee type: jitclass.EmbedPosition#123e539b0<embed:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,w:array(float64, 2d, A),params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>>\n","During: typing of call at /Users/camillaandas/Documents/Dokumenter – Camillas MacBook Air/vitber/Vitber-indmat-1/neural_network.py (22)\n","\n","\n","File \"neural_network.py\", line 22:\n","    def __init__(self, r: int = 5, d: int = 10, m: int = 2, L: int = 5, p: int = 128, k: int = 8):\n","        <source elided>\n","        n_max = 2 * r - 1\n","        self.embedding = EmbedPosition(n_max, m, d, 0.1)\n","        ^\n","\n","During: resolving callee type: jitclass.NeuralNetwork#123d29850<embedding:instance.jitclass.EmbedPosition#123e539b0<embed:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,w:array(float64, 2d, A),params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>>,transformer_blocks:list(instance.jitclass.TransformerBlock#123e703e0<self_attention:instance.jitclass.SelfAttention#123e2e3f0<W_q:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_k:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_v:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_o:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,prev_A:OptionalType(array(float64, 3d, A)),softmax:instance.jitclass.Softmax#123e2eab0<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,attention:instance.jitclass.Attention#123e2f140<softmax:instance.jitclass.Softmax#123e2eab0<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,matmul1:instance.jitclass.Matmul#123e2ea20<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>,matmul2:instance.jitclass.Matmul#123e2ea20<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>>>,feed_forward:instance.jitclass.FeedForward#123e53a40<l1:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,activation:instance.jitclass.Relu#123e51d60<x:array(float64, 3d, A)>,l2:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,x:OptionalType(array(float64, 3d, A))>>)<iv=None>,lm_head:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,out_softmax:instance.jitclass.Softmax#123e2eab0<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>>\n","During: typing of call at <string> (3)\n","\n","During: resolving callee type: jitclass.NeuralNetwork#123d29850<embedding:instance.jitclass.EmbedPosition#123e539b0<embed:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,w:array(float64, 2d, A),params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>>,transformer_blocks:list(instance.jitclass.TransformerBlock#123e703e0<self_attention:instance.jitclass.SelfAttention#123e2e3f0<W_q:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_k:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_v:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,W_o:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,prev_A:OptionalType(array(float64, 3d, A)),softmax:instance.jitclass.Softmax#123e2eab0<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,attention:instance.jitclass.Attention#123e2f140<softmax:instance.jitclass.Softmax#123e2eab0<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>,matmul1:instance.jitclass.Matmul#123e2ea20<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>,matmul2:instance.jitclass.Matmul#123e2ea20<prev_A:OptionalType(array(float64, 3d, A)),prev_B:OptionalType(array(float64, 3d, A))>>>,feed_forward:instance.jitclass.FeedForward#123e53a40<l1:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,activation:instance.jitclass.Relu#123e51d60<x:array(float64, 3d, A)>,l2:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,x:OptionalType(array(float64, 3d, A))>>)<iv=None>,lm_head:instance.jitclass.LinearLayer#123e2dd30<params:DictType[unicode_type,DictType[unicode_type,array(float64, 2d, A)]<iv=None>]<iv=None>,x:array(float64, 3d, A),has_bias:bool>,out_softmax:instance.jitclass.Softmax#123e2eab0<epsilon:float64,prev_Q:OptionalType(array(float64, 3d, A)),prev_P:OptionalType(array(float64, 3d, A)),prev_z_l:OptionalType(array(float64, 3d, A))>>\n","During: typing of call at <string> (3)\n","\n","\n","File \"<string>\", line 3:\n","<source missing, REPL/exec in use?>\n","\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n","    frames.append(self.format_record(record))\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n","    frame_info.lines, Colors, self.has_colors, lvals\n","    ^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/IPython/core/ultratb.py\", line 792, in lines\n","    return self._sd.lines\n","           ^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/stack_data/core.py\", line 698, in lines\n","    pieces = self.included_pieces\n","             ^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/stack_data/core.py\", line 649, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","                             ^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/stack_data/core.py\", line 628, in executing_piece\n","    return only(\n","           ^^^^^\n","  File \"/Users/camillaandas/micromamba/envs/3.12/lib/python3.12/site-packages/executing/executing.py\", line 164, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]}],"source":["n = 100\n","\n","test_model_1 = make_model(r=5, d=10, m=2, L=2, p=15, k=5)\n","test_model_2 = make_model(r=7, d=20, m=5, L=2, p=25, k=10)\n","\n","sorting_data1 = get_train_test_sorting(5, 2, 250, 10, 10)\n","trained_model1, mean_loss_arr1 = training_sorting(test_model_1, CrossEntropy(), Adam(), sorting_data1, 2, n_epochs=n, r=5)\n","\n","sorting_data2 = get_train_test_sorting(7, 5, 250, 20, 20)\n","trained_model2, mean_loss_arr2 = training_sorting(test_model_2, CrossEntropy(), Adam(), sorting_data2, 5, n_epochs=n, r=7)\n","\n","iter = np.arange(n)\n","\n","plt.title(\"Modell 1\")\n","plt.plot(iter, mean_loss_arr1)\n","plt.show()\n","plt.title(\"Modell 2\")\n","plt.plot(iter, mean_loss_arr2)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy1 = test_sorting(trained_model1, sorting_data1, m=2)\n","print(f'Accuracy for model 1: {np.round(accuracy1, 1)}%')\n","\n","accuracy2 = test_sorting(trained_model2, sorting_data2, m=5)\n","print(f'Accuracy for model 2: {np.round(accuracy2, 1)}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n = 100\n","\n","test_model_add = make_model(r=6, d=30, m=10, L=3, p=40, k=20)\n","addition_data = get_train_test_addition(n_digit=2, samples_per_batch=250, n_batches_train=20, n_batches_test=20)\n","\n","trained_model_add, mean_loss_arr_add = training_addition(test_model_add, CrossEntropy(), Adam(), addition_data, 10, n_epochs=n, r=2)\n","\n","iter = np.arange(n)\n","\n","plt.title(\"Addition loss\")\n","plt.plot(iter, mean_loss_arr_add)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy_addition = test_addition(trained_model_add, addition_data, m=10)\n","print(f'Accuracy for model: {np.round(accuracy_addition, 1)}%')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.12.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":2}
